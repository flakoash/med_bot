{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc8b56a",
   "metadata": {},
   "source": [
    "# Research candidate LLMs for RAG\n",
    "\n",
    "Based on the findings in our EDA, we decided that we will finetune an LLM model for question answering, in this notebook we will explore some alternatives (restricted by the current available GPU for development RTX 3060ti). We will analyse and explore alternatives and best configurations (quantization, PEFT training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5932ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from bert_score import score as bertscore\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b99f3",
   "metadata": {},
   "source": [
    "# define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855093d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluations(preds, targets):\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=targets, use_stemmer=True)\n",
    "\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    P, R, F1, _ = bertscore.compute(predictions=preds, references=targets, lang=\"en\", model_type=\"distilbert-base-uncased\").values()\n",
    "    \n",
    "    bertscore_avg = {\n",
    "        \"bertscore_precision\": np.array(P).mean().item(),\n",
    "        \"bertscore_recall\": np.array(R).mean().item(),\n",
    "        \"bertscore_f1\": np.array(F1).mean().item(),\n",
    "    }\n",
    "    \n",
    "    return {**rouge_scores, **bertscore_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e352fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = compute_evaluations([\"if you have the flu you need to rest\"], [\"rest is the best for the flu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992b6387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.375),\n",
       " 'rouge2': np.float64(0.14285714285714288),\n",
       " 'rougeL': np.float64(0.25),\n",
       " 'rougeLsum': np.float64(0.25),\n",
       " 'bertscore_precision': 0.744418740272522,\n",
       " 'bertscore_recall': 0.7576205730438232,\n",
       " 'bertscore_f1': 0.7509616613388062}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86104718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('rouge1', np.float64(0.375)), ('rouge2', np.float64(0.14285714285714288)), ('rougeL', np.float64(0.25)), ('rougeLsum', np.float64(0.25)), ('bertscore_precision', 0.744418740272522), ('bertscore_recall', 0.7576205730438232), ('bertscore_f1', 0.7509616613388062)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd66e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1, rouge2, rougeL, rougeLsum, bertscore_precision,bertscore_recall, bertscore_f1= m.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab21f836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.0),\n",
       " 'rouge2': np.float64(0.0),\n",
       " 'rougeL': np.float64(0.0),\n",
       " 'rougeLsum': np.float64(0.0),\n",
       " 'bertscore_precision': 0.6455554366111755,\n",
       " 'bertscore_recall': 0.6872128844261169,\n",
       " 'bertscore_f1': 0.6657330989837646}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_evaluations([\"if you have the flu you need to rest\"], [\"I like pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfd3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.000000238418579, 1.000000238418579, 0.6719009876251221],\n",
       " [1.000000238418579, 1.000000238418579, 0.6600361466407776],\n",
       " [1.000000238418579, 1.000000238418579, 0.6659157276153564],\n",
       " 'distilbert-base-uncased_L5_idf_version=0.3.12(hug_trans=4.53.1)')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\", \"if you have the flu you need to rest\"]\n",
    "references = [\"hello there\", \"general kenobi\", \"I like pizza but I dont like mangos\"]\n",
    "P, R, F1, _  = bertscore.compute(predictions=predictions, references=references, lang=\"en\", idf=True, model_type=\"distilbert-base-uncased\").values()\n",
    "P, R, F1, _ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb0e39",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "as we can see here we can not blindly trust bert-scored specially on small inputs but it should still be a good metrict to help us compare semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44356f3",
   "metadata": {},
   "source": [
    "# test some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c29c77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_words</th>\n",
       "      <th>valid_question</th>\n",
       "      <th>valid_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>what research (or clinical trials) is being do...</td>\n",
       "      <td>New types of treatment are being tested in cli...</td>\n",
       "      <td>249</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>What is (are) Leg Injuries and Disorders ?</td>\n",
       "      <td>Your legs are made up of bones, blood vessels,...</td>\n",
       "      <td>96</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>How to diagnose Axenfeld-Rieger syndrome type 1 ?</td>\n",
       "      <td>Is genetic testing available for Axenfeld Rieg...</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>What is (are) Kidney Disease ?</td>\n",
       "      <td>When your kidneys fail, they are no longer abl...</td>\n",
       "      <td>82</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>What is (are) Ovarian Germ Cell Tumors ?</td>\n",
       "      <td>Key Points\\n                    - Ovarian germ...</td>\n",
       "      <td>237</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "910   what research (or clinical trials) is being do...   \n",
       "1692         What is (are) Leg Injuries and Disorders ?   \n",
       "2722  How to diagnose Axenfeld-Rieger syndrome type 1 ?   \n",
       "239                      What is (are) Kidney Disease ?   \n",
       "1334           What is (are) Ovarian Germ Cell Tumors ?   \n",
       "\n",
       "                                                 answer  answer_words  \\\n",
       "910   New types of treatment are being tested in cli...           249   \n",
       "1692  Your legs are made up of bones, blood vessels,...            96   \n",
       "2722  Is genetic testing available for Axenfeld Rieg...            64   \n",
       "239   When your kidneys fail, they are no longer abl...            82   \n",
       "1334  Key Points\\n                    - Ovarian germ...           237   \n",
       "\n",
       "      valid_question  valid_answer  \n",
       "910             True          True  \n",
       "1692            True          True  \n",
       "2722            True          True  \n",
       "239             True          True  \n",
       "1334            True          True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_parquet(\"../data/cleaned/validation_dataset.parquet\")\n",
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a502bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models = [#(\"text2text-generation\", \"google/flan-t5-base\"), \n",
    "                    (\"text-generation\",\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"), \n",
    "                    (\"text-generation\", \"unsloth/Phi-3.5-mini-instruct\")]\n",
    "\n",
    "for task, model_id in candidate_models:\n",
    "    qa_model = pipeline(task, model=model_id)\n",
    "    if task == \"text-generation\":\n",
    "        input = [{\"role\": \"user\", \"content\": \"What is the flu?\"}]\n",
    "    else:\n",
    "        input = \"What is the flu?\"\n",
    "    response = qa_model(input)[0][\"generated_text\"]\n",
    "    print(f\"model: {model_id} -> response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8569c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def evaluate_qa_models(\n",
    "    df: pd.DataFrame,\n",
    "    qa_model: Callable[[str], str],\n",
    "    experiment_name: str,\n",
    "    model_name: str,\n",
    "    mlflow_uri: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs QA model over df and logs EM & F1 metrics to MLflow.\n",
    "    \"\"\"\n",
    "    if mlflow_uri:\n",
    "        mlflow.set_tracking_uri(mlflow_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"num_examples\", len(df))\n",
    "\n",
    "        preds = []\n",
    "        truths = []\n",
    "        for _, row in df.iterrows():\n",
    "            q = row[\"question\"]\n",
    "            input = [{\"role\": \"user\", \"content\": q}]\n",
    "            truth = row[\"answer\"]\n",
    "            pred = qa_model(input)\n",
    "            preds.append(pred)\n",
    "            truths.append(truth)\n",
    "\n",
    "        metrics = compute_evaluations(preds, truths)\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # log predictions and truth as artifact for inspection\n",
    "        out_df = df.copy()\n",
    "        out_df[\"predicted\"] = preds\n",
    "\n",
    "\n",
    "        os.makedirs(\"mlflow_artifacts\", exist_ok=True)\n",
    "        csv_path = \"mlflow_artifacts\" / f\"{model_name}_predictions.csv\"\n",
    "        out_df.to_csv(csv_path, index=False)\n",
    "        mlflow.log_artifact(str(csv_path), artifact_path=\"predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models = [\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", \n",
    "                    \"unsloth/Phi-3.5-mini-instruct\"]\n",
    "\n",
    "for model_id in candidate_models:\n",
    "    qa_model = pipeline(\"text-generation\", model=model_id)\n",
    "    evaluate_qa_models(\n",
    "        df = val_df.head(10), \n",
    "        qa_model=qa_model,\n",
    "        experiment_name=\"initial comparision\",\n",
    "        model_name=model_id\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_bot",
   "language": "python",
   "name": "rag-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
